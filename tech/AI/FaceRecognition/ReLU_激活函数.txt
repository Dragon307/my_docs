
----------------
Relu 激活函数
----------------

----------------------------------------------------------------------------------------

RELU 激活函数及其他相关的函数

http://blog.csdn.net/u013146742/article/details/51986575

介绍了Relu 激活函数的优缺点，同时提供了Relu的改进函数 Leaky-ReLU、P-ReLU、R-ReLU 。

---------------------------

ReLU

近年来，ReLU 变的越来越受欢迎。它的数学表达式如下： 

    f(x) = max(0, x)

很显然，从图左可以看出，输入信号<0时，输出都是0，>0 的情况下，输出等于输入。

---------------------------

ReLU 的优点：

Krizhevsky et al. 发现使用 ReLU 得到的SGD的收敛速度会比 sigmoid/tanh 快很多(看右图)。有人说这是因为它是linear，而且 non-saturating
相比于 sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。
ReLU 的缺点： 当然 ReLU 也有缺点，就是训练的时候很”脆弱”，很容易就”die”了. 什么意思呢？

举个例子：一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了。
如果这个情况发生了，那么这个神经元的梯度就永远都会是0.

实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都”dead”了。 
当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁。

----------------------------------------------------------------------------------------

ReLu(Rectified Linear Units) 激活函数

http://www.cnblogs.com/neopenx/p/4453161.html

近似生物神经激活函数：Softplus & ReLu

2001年，神经科学家 Dayan、Abott 从生物学角度，模拟出了脑神经元接受信号更精确的激活模型。

Softplus 函数是 Logistic-Sigmoid 函数原函数。

    Softplus(x) = log(1 + e^x)

--------------------------

生物神经的稀疏激活性。

--------------------------

Relu 激活函数的贡献：

ReLu的使用，使得网络可以自行引入稀疏性。这一做法，等效于无监督学习的预训练。


当然，效果肯定没预训练好。论文中给出的数据显示，没做预训练情况下，ReLu激活网络遥遥领先其它激活函数。

甚至出现了比普通激活函数预训练后更好的奇葩情况。当然，在预训练后，ReLu仍然有提升空间。

从这一层面来说，ReLu缩小了非监督学习和监督学习之间的代沟。当然，还有更快的训练速度。

----------------------------------------------------------------------------------------

常用激活函数的总结与比较

https://livc.io/176

Maxout

Maxout 是对 ReLU 和 Leaky ReLU 的一般化归纳，它的函数公式是（二维时）：

  f(x) = max(w1^T + b1, W2^T + b2)。ReLU 和 Leaky ReLU 都是这个公式的特殊情况（比如 ReLU 就是当 w1, b1 = 0 时）。
 
这样 Maxout 神经元就拥有 ReLU 单元的所有优点（线性和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和 ReLU 对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。

----------------------------------------------------------------------------------------

综合上面几篇文章，我自己设计了一个激活函数。

Randomized Leaky-ReLU-Tanh 激活函数：

    f(x) = tanh(x)，(x >= 0)
    f(x) = α * x， (x < 0)，(α是一个极小的随机值，以防止“dying ReLU”问题)

tanh 激活函数和 Leaky-ReLU 激活函数的组合。

----------------------------------------------------------------------------------------

神经网络之激活函数 dropout 原理解读 BatchNormalization 代码实现

http://blog.csdn.net/u014696921/article/details/53749831

这篇文章较上面的几篇文章来看，内容是相似的，当更完整，还介绍了 dropout 。

--------------------------

开篇明义，dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。

dropout是CNN中防止过拟合提高效果的一个大杀器，但对于其为何有效，却众说纷纭。在下读到两篇代表性的论文，代表两种不同的观点，特此分享给大家。

----------------------------------------------------------------------------------------

理解dropout

http://blog.csdn.net/stdcoutzyx/article/details/49022443

这是上文中转载的原文。

----------------------------------------------------------------------------------------
